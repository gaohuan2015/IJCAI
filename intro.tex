\section{Introduction}
Recent advances in information extraction have led to huge Knowledge graphs(KGs), such as DBpedia~\cite{DBpedia2007}, YAGO~\cite{Yago2007} and NELL~\cite{NELL2015}. These KGs contain facts which represent relations between entities as triples $<h,r,t>$. A triple indicates that entities $h$ and $t$ are connected by relation $r$. Even a KG contains a very large number of triples, it is still far from complete. The completeness of KGs damages their usefulness in downstream real applications. Knowledge graph completion or link predictions is thus important approaches for populating existing KGs.

Knowledge graph embedding models for KG completion have attracted much attention, due to their outstanding performance on various applications that require machines to recognize and understand queries and their semantics. These embedding models are to represent entitles and relations in a KG into a low dimensional continuous vector space, where these vectors contain rich semantic information, and can benefit many downstream tasks especially knowledge graph completion or linked predictions.

Despite the success of previous approaches in KG embedding ~\cite{BordesUGWY13} ~\cite{WangZFC14}, most of the mainly models treat triples individually, ignoring lots of information implicitly provided by the structure of the KG. In fact, triples and the relations among them have abundant information that can be further used for inference. Recently, several authors have addressed this issue by incorporating relation path information into model learning ~\cite{LinLLSRL15} ~\cite{Toutanova16} learning and have shown that the relation paths between entities in KGs provide useful information and improve the task of KG completion. These approaches only consider relation information while missing more structure information which contain rich clues for inference. For instance if we know that Ben Affleck has won an Oscar award and Ben Affleck lives in Los Angeles, then this can help us to predict that Ben Affleck is an actor or a film maker, rather than a lecturer or a doctor.

In order to utilize the structure information, we present a novel approach to embed a knowledge graph named Attentional-Triple-Context-based Knowledge Embedding model(ATCE). ATCE utilizes and chooses the proper context of each triple in the knowledge graph. We define triple context consisting of neighbor context and path context, and define a new score function to evaluate the correlation between a triple and its contexts. Instead of using each triple independently, we incorporate triple context into the score function which is used to evaluate the confidence of a triple. 

The advantages of our approach are three-fold:
1) We embed a triple by utilizing a local subgraph around a triple instead of a set of independent triples, and extract two kinds of contexts named triple context.

2) Based on the triple context, we proposed a novel embedding learning approach which named ATCE and a new loss function which converts the score function in TransE to a conditional probability.

3) In order to overcome the noisy data in the triple context of a triple, an attention mechanism in our approach is proposed to choose the proper information for embedding. In the meanwhile, the attention mechanism can learn the representation power of different neighbor entities and connective from in its context.

Finally, we have conducted preliminary experiments on two benchmark data sets and assessed our method on link prediction task and triple classification. In the experiments we show chosen context through the attention mechanism to improve the effectiveness of this mechanism. The experimental results show impressive improvements on predictive accuracy compared to other baselines which include TransE~\cite{BordesUGWY13}, TransH~\cite{WangZFC14}, TransR~\cite{LinLSLZ15} , CTransR~\cite{LinLSLZ15}, PTransE~\cite{LinLLSRL15} and GAKE~\cite{FengHYZ16}. .

