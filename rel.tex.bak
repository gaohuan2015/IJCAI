\section{Related Work}\label{sec:rel}
In recent year, a new kind of approach named \emph{knowledge graph embedding} (also known as \emph{knowledge representation learning}) has been studied in~\cite{NickelTK11}\cite{SocherCMN13}\cite{BordesUGWY13}\cite{WangZFC14}\cite{LinLSLZ15}. It makes KGs more manageable and performs well on several important tasks like link prediction~\cite{BordesWCB11} and triple classification~\cite{SocherCMN13}.  It aims to embed a KG into a low dimensional vector space, where each entity and relation are represented as a vector (also referred to as \emph{embedding}) or a matrix. In general, a KG embedding model defines a score function for evaluating the confidence of a triple, and optimizes an objective function constructed from the score function to learn the embeddings. In this way, symbolic computation can be replaced by numerical computation, which makes it easier to manipulate KGs through linear algebra operations while the semantic information is retained meanwhile. A few typical models of knowledge graph embedding are as follows. TransE is one of the most widely used approach in those approaches, which views a head entity can be translation to a tail entity by a relationship on the same hyperplane. Based on the score function of the translation model which is designed as£º$f_{TransE}(h,r,t)=\|\bm{\mathrm{h}} + \bm{\mathrm{r}} -\bm{\mathrm{t}}\|_{L_1/L_2}$£¬variant models of TransE~\cite{BordesWCB11} are introduced to improve the performance. In TransH~\cite{WangZFC14}, entities are projected into the relation specific hyperplane that is perpendicular to the relationship embedding. TransD~\cite{BordesWCB11} and TransR~\cite{BordesWCB11} still follow the principle of TransH, the entities and relations are embedded on different hyperplane by the different mapping matrices which are both related to the entity and relation.

Translation-based approaches treat knowledge graph as a set of triples. Unlike aforementioned models, TransA~\cite{BordesWCB11} utilizes a closed set of candidate entities to determine the relation. The similar works include RESCAL,SME and LFM. In these models, relations are represented as a bilinear operator that can interactions across the entities of a triples to explain its validity.  In addition,some works discover incorporating additional information will improve the performance, such as text~\cite{BordesWCB11} and  entity type~\cite{BordesWCB11}.

There is also works that incorporates graph structures into KG embedding models. PTransE~\cite{LinLLSRL15} is a path-based representation learning model, which utilizes relation paths to improve the performance of TransE. However, it merely uses relation paths in the KG, ignoring other information contained in graph structures. Besides, GAKE~\cite{FengHYZ16}, the most relevant model for our work, leverages three types of graph context for representation learning, in a way similar to learning language models. However, when dealing with contexts, it average the embeddings of all entities and relations in a context as the representation of it, in which way the structural information is still lost. Furthermore, its experimental results are inferior to TransH and TransR. It is a huge challenge in context selection and the feasible use of it.
