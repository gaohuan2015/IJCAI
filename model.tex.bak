\section{Knowledge Graph Embedding With Attentional Triple Context} \label{sec:pro}
So far, we have introduced neighbor context and path context, based on which we can define triple context. The triple context of triple $(h,r,t)$ is composed of the neighbor context of the head entity $h$ , the path context of the entity pair $(h,t)$, which can be formalized as:
\begin{equation}\label{triple context}
  C(h,r,t) = C_N(h) \cup C_P(h, t)
\end{equation}

The triple context of a triple can be considered to embody the surrounding structures of it in the graph, which makes the model aware of the information contained in graph structures.

We then introduce our approach in detail. In general KG embedding models, the score function of a triple is only related to the embeddings of entities and relations. For example, TransE defines the score function as $f_{TransE}(h,r,t)=\|\bm{\mathrm{h}} + \bm{\mathrm{r}} -\bm{\mathrm{t}}\|_{L_1/L_2}$. In our method, triple context is introduced in the score function. Given a candidate triple $(h,r,t)$, the score function is the conditional probability that the triple holds given the triple context and all the embeddings, as follows:
\begin{equation}\label{score_function}
  f(h,r,t) = P((h,r,t)|C(h,r,t);\Theta)
\end{equation}
where $C(h,r,t)$ is the triple context of $(h,r,t)$. A higher score of a triple indicates that it holds to a greater extent.

We define an objective function by maximizing the joint probability of all triples in knowledge graph $\mathcal{K}$, which can be formulated as:
\begin{align} \label{joint_prob}
  P(\mathcal{K}|\Theta) &= \prod_{(h,r,t)\in \mathcal{K}} f(h,r,t)
\end{align}

For the score function in Eq.~\eqref{score_function}, we use conditional probability formula to decompose the probability $P((h,r,t)|C(h,r,t);\Theta)$ as:
\begin{align} \label{decomposition}
  \begin{split}
    f(h,r,t) &= P(h|C(h,r,t);\Theta) \\
    & \cdot P(t|C(h,r,t),h;\Theta) \\
    & \cdot P(r|C(h,r,t),h,t;\Theta)
  \end{split}
\end{align}
where the evaluation of the whole triple is decomposed into three parts. The probabilities that $h$, $t$ and $r$ appear given respective condition are determined in turn in these three parts.
% $h$ is determined first based on triple context, then $t$ is determined based on $h$ and triple context, and finally $r$ likewise.

The first part $P(h|C(h,r,t);\Theta)$ in Eq.~\eqref{decomposition} represents the conditional probability that $h$ is the head entity given the triple context and all the embeddings. Since whether $h$ appears is decided mostly by the neighboring structures of $h$ in the KG, we can approximate $P(h|C(h,r,t);\Theta)$ as $P(h|C_N(h);\Theta)$, where $C_N(h)$ is the \emph{neighbor context} of $h$ in the KG. The approximated probability $P(h|C_N(h);\Theta)$ can be considered as the compatibility between $h$ and its neighbor context, it is formalized as a softmax-like representation, which is also used in~\cite{DBLP:conf/emnlp/WangZFC14} and has been validated, as follows:
\begin{align} \label{P_h}
  P(h|C_N(h);\Theta) = \frac {\exp(g_N(h, C_N(h)))} {\sum_{h' \in \mathcal{E}} \exp(g_N(h', C_N(h)))}
\end{align}
where $g_N(h', C_N(h))$ is the function that describes the correlation between an arbitrary entity $h'$ and entity context of the specific entity $h$. In reality, different neighbors may have different power of influence to represent $h$. In order to obtain the correlation $g_N(h', C_N(h))$, we first obtain $a_i$ which means the correlation between the $ith$ neighbor entity in $C_N(h)$ and the triple $<h',r,t>$. Inspired by score function of TransE, we substitute the neighbor entities in $C_N(h)$ for the head $h'$ in triple and then we define $a_i$ as
$$
a_i = \|t_{n_i} - r_{n_i} + r - t\|
$$
Where $t_{n_i}$ is the $ith$ neighbor entity in $C_N(h)$ and $r_{n_i}$ is the relation between $t_{n_i}$ and $h'$.

Then We use attention model $a(C_N(h))$ to represent how $h'$ selectively focuses on $C_N(h)$.
$$
\alpha_i = \frac {\exp(-a_i)} {\sum_{j} \exp(-a_j)}
$$

Finally, based on the attention results we obtain the correlation between an arbitrary entity $h'$ and neighbor context $C_N(h)$:
$$
g_N(h¡¯, C_N(h)) = -\sum_{i} \alpha_{i} \|\mathbf{h'} + \mathbf{r_{n_i}} - \mathbf{t_{n_i}}\|
$$

The second part $P(r,t|C(h,r,t), h; \Theta)$ in Eq.~\eqref{decomposition} is the conditional probability that $t$ is the tail entity and $r$ is the relation given the head entity $h$, triple context and all the embeddings. In this part we introduce path context that means $t$ could be related to $h$ through a potential relation, which can be considered as the relatedness between $h$ and $t$. We use path context between $h$ and $t$ to measure the relatedness of them and approximate $P(r,t|C(h,r,t),h;\Theta)$ as $P(t|C_P(h,t),h;\Theta)$, where $C_P(h,t)$ is the \emph{path context} between $h$ and $t$. The approximated probability $P(r,t|C_P(h,t),$ $h;\Theta)$ is formalized as follows:
\begin{align}\label{P_t}
P(r,t|C_P(h, t), h;\Theta) = \frac {\exp(g_P(r,t, C_P(h, t)))} {\sum_{r' \in \mathcal{R}, t' \in \mathcal{E}} \exp(g_P(r', t', C_P(h, t)))}
\end{align}

where $g_P(\cdot, \cdot)$ is a function of correlation among an arbitrary entity $t'$ , an arbitrary relation $r$ and path context of the specific entity pair $(h,t)$. Similar to the neighbor context, given a triple $<h,r,t>$ different pathes in a path context $C_P(h, t)$ have different power of influence the triple. In order to obtain $g_P(\cdot, \cdot)$, we firstly calculate the correlation $b_i$ between path $p_i$ in $C_P(h, t)$ and the triple:
\begin{equation}\label{f2}
b_i = \|\mathbf{h} + \mathbf{p_i} - \mathbf{t}\|
\end{equation}

where $\bm{\mathrm{p_i}}$ composes all relations in $p_i$ into a single vector by summing over all their embeddings. For example, for path $p_i=(r_{m_1}, \cdots, r_{m_i})$, the embedding of it is $\bm{\mathrm{p}}_i = \bm{\mathrm{r}}_{m_1} + \cdots + \bm{\mathrm{r}}_{m_i}$. Eq.~\eqref{f2} has a similar meaning with Eq.~\eqref{f1}.

We then choose important pathes from $C_P(h, t)$ through the attention mechanisms. For each $p_i$, the weight $\beta_{i}$ is then defined as 
$$
\beta_{i} = \frac {\exp(-b_i)} {\sum_j \exp(-b_j)}
$$

Finally given an  arbitrary entity $t'$ and an arbitrary relation $r'$, the correlation $g_P(\cdot, \cdot)$ between $t'$,$r'$ and $C_P(h, t)$ is:
\begin{equation}\label{gp}
g_P(r', t', C_P(h,t)) = -\sum_{i} \beta_i  (\|\mathbf{h} + \mathbf{p}_i - \mathbf{t'} \| + \|\mathbf{p}_i - \mathbf{r'}\|)
\end{equation}

In Eq.~\eqref{gp}, $\|\mathbf{h} + \mathbf{p}_i - \mathbf{t'} \|$ indicates the correlation between $t'$ and $p_i$. Similarly£¬$|\mathbf{p}_i - \mathbf{r'}\|$ is the correlation between $r'$ and $p_i$. 


For computational convenience, the joint probability in Eq.~\eqref{joint_prob} is transformed to a negative logarithmic loss function, which can be optimized by \emph{stochastic gradient descent} (SGD).

1: **initialize** $\mathbf{r} \leftarrow uniform(- 6/\sqrt{k}, 6/\sqrt{k})$ for each $r \in \mathcal{R}$

2:                  $\mathbf{r} \leftarrow \mathbf{r}/\|\mathbf{r}\|$ for each $r \in \mathcal{R}$

3:                  $\mathbf{e} \leftarrow uniform(-6/\sqrt{k}, 6/\sqrt{k})$ for each entity $e \in \mathcal{E}$

4: **loop** // one epoch

5:         $\mathbf{e} \leftarrow \mathbf{e} / \|\mathbf{e}\|$ for each entity $e \in \mathcal{E}$

6:         **for** each batch $S_{batch}$  in $\mathcal{K}$ // one batch

7:                 **for** $(h, r, t) \in S_{batch}$

8:                         construct a set of corrupted triples $S'_{(h,r,t)} = \{(h', r', t')\}$

9:                 **end for**

10:               update embedding w.r.t $\sum_{(h,r,t) \in S_{batch}} \nabla[\log \sigma(g_N(h, C_N(h))) + \sum_{(h', r', t') \in S'_{(h, r, t)}} \log \sigma(-g_N(h', C_N(h))) + \log \sigma(g_P(r, t, C_P(h, t))) + \sum_{(h', r', t') \in S'_{(h, r, t)}} \sigma(- g_P(r', t', C_P(h, t)))]$

11:       **end for**

12: **end loop**
\subsection{Model Learning}
By feasible approximation, the score function is transformed to Eq.~\eqref{decomposition_approx}, each part is represented in softmax form as Eq.~\eqref{P_h}, Eq.~\eqref{P_t} and Eq.~\eqref{P_r}. However, it is impractical to compute these softmax functions directly because of high computational overhead. Hence, we adopt negative sampling, which is proposed in \cite{DBLP:conf/nips/MikolovSCCD13} to approximate full softmax function efficiently, to approximate softmax functions in our model. Taking $P(h|C_N(h);\Theta)$ in Eq.\eqref{P_h} as an example, it is approximated via negative sampling as follows:
\begin{equation}\label{approximation}
  %\begin{split}
    P(h|C_N(h);\Theta) \approx \sigma(f_1(h, C_N(h))) \cdot \prod_{(h',r,t) \in \mathcal{K}'}^{n} \sigma(f_1(h', C_N(h)))
  %\end{split}
\end{equation}
where $\mathcal{K}' = \{h', r, t\}$ is the corrupted triples by replacing the head entity with an arbitrary entity, $n$ is the number of negative samples and $\sigma(\cdot)$ is the logistic function. $P(t|C_P(h,t),h;\Theta)$ in Eq.~\eqref{P_t} and $P(r|h,t;\Theta)$ in Eq.~\eqref{P_r} are approximated likewise.

In real data sets, the size of neighbor context and path context may be very large, which is computationally expensive for model learning. For this reason, we sample from neighbor context and path context to make triple context tractable. Specifically, we set a threshold $n_N$ for neighbor context and $n_P$ for path context; if the size of the original context exceeds the threshold, we sample a subset, size of which is the threshold, for model learning. Moreover, the length of relation path is constrained to 2 and 3 in our model.

\begin{equation}
\begin{split}
-\log P(\mathcal{K} | \Theta)
& = -\sum_{(h,r,t) \in \mathcal{K}} [\log \sigma(g_N(h, C_N(h))) \\
&+ \sum_{h'} \log \sigma(-g_N(h', C_N(h))) \\
&+ \log \sigma(g_P(r, t, C_P(h, t))) \\
&+ \sum_{r', t'} \sigma(- g_P(r', t', C_P(h, t)))]
\end{split}
\end{equation}

\begin{equation}
\begin{split}
P(\mathcal{K}|\Theta) &= \prod_{(h,r,t) \in \mathcal{K}} f(h,r,t) \\
&= \prod_{(h, r, t) \in \mathcal{K}} {P(h|C(h,r,t);\Theta) \cdot P(r,t|C(h,r,t), h; \Theta)} \\
&\approx \prod_{(h, r, t) \in \mathcal{K}} [\sigma(g_N(h, C_N(h))) \cdot \prod_{h'} \sigma(-g_N(h', C_N(h)))] \\
&\cdot [\sigma(g_P(r, t, C_P(h, t))) \cdot \prod_{r', t'} \sigma(- g_P(r', t', C_P(h, t)))]
\end{split}
\end{equation}

\begin{comment}
the objective function is formulated as follows:
\begin{equation}\label{cost_function}
  \mathcal{L}(\mathcal{K}) = \sum_{(h,r,t)\in\mathcal{K}}
\end{equation}

For computational convenience, the original joint probability in Eq.~\eqref{joint_prob} is transformed into the negative logarithmic form, which can be optimized by stochastic gradient descent (SGD). The cost function of a triple $(h,r,t)$ is formulated as:
\begin{equation}\label{score_function}
*
\end{equation}
\end{comment}

