\section{Knowledge Graph Embedding With Attentional Triple Context} \label{sec:pro}
So far, we have introduced neighbor context and path context, based on which we can define triple context. The triple context of triple $(h,r,t)$ is composed of the neighbor context of the head entity $h$ , the path context of the entity pair $(h,t)$, which can be formalized as:
\begin{equation}\label{triple context}
  C(h,r,t) = C_N(h) \cup C_P(h, t)
\end{equation}

The triple context of a triple can be considered to embody the surrounding structures of it in the graph, which makes the model aware of the information contained in graph structures.

We then introduce our approach in detail. In general KG embedding models, the score function of a triple is only related to the embeddings of entities and relations. For example, TransE defines the score function as $f_{TransE}(h,r,t)=\|\bm{\mathrm{h}} + \bm{\mathrm{r}} -\bm{\mathrm{t}}\|_{L_1/L_2}$. In our method, triple context is introduced in the score function. Given a candidate triple $(h,r,t)$, the score function is the conditional probability that the triple holds given the triple context and all the embeddings, as follows:
\begin{equation}\label{score_function}
  f(h,r,t) = P((h,r,t)|C(h,r,t);\Theta)
\end{equation}
where $C(h,r,t)$ is the triple context of $(h,r,t)$. A higher score of a triple indicates that it holds to a greater extent.

We define an objective function by maximizing the joint probability of all triples in knowledge graph $\mathcal{K}$, which can be formulated as:
\begin{align} \label{joint_prob}
  P(\mathcal{K}|\Theta) &= \prod_{(h,r,t)\in \mathcal{K}} f(h,r,t)
\end{align}

For the score function in Eq.~\eqref{score_function}, we use conditional probability formula to decompose the probability $P((h,r,t)|C(h,r,t);\Theta)$ as:
\begin{align} \label{decomposition}
  \begin{split}
    f(h,r,t) &= P(h|C(h,r,t);\Theta) \\
    & \cdot P(t,r|C(h,r,t),h;\Theta)
  \end{split}
\end{align}
where the evaluation of the whole triple is decomposed into two parts. The probabilities that $h$, $t$ and $r$ appear given respective condition are determined in turn in these two parts.
% $h$ is determined first based on triple context, then $t$ is determined based on $h$ and triple context, and finally $r$ likewise.

The first part $P(h|C(h,r,t);\Theta)$ in Eq.~\eqref{decomposition} represents the conditional probability that $h$ is the head entity given the triple context and all the embeddings. Since whether $h$ appears is decided mostly by the neighboring structures of $h$ in the KG, we can approximate $P(h|C(h,r,t);\Theta)$ as $P(h|C_N(h);\Theta)$, where $C_N(h)$ is the \emph{neighbor context} of $h$ in the KG. The approximated probability $P(h|C_N(h);\Theta)$ can be considered as the compatibility between $h$ and its neighbor context, it is formalized as a softmax-like representation, which is also used in~\cite{DBLP:conf/emnlp/WangZFC14} and has been validated, as follows:
\begin{align} \label{P_h}
  P(h|C_N(h);\Theta) = \frac {\exp(g_N(h, C_N(h)))} {\sum_{h' \in \mathcal{E}} \exp(g_N(h', C_N(h)))}
\end{align}
where $g_P(\cdot, \cdot)$ is the function that describes the correlation between an arbitrary entity $h'$ and a neighbor context of the specific entity $h$. In reality, different neighbors may have different power of influence to represent $h$. For example, a target entity is $Terminate2:JudgementDay$ which is a famous movie in 1991. The director of this movie or the type of this movie may more important than others. In order to filter unrelated entities and obtain $g_P(\cdot, \cdot)$, we first obtain $a_i$ which means the represent power between the $ith$ neighbor entity in $C_N(h)$ and an arbitrary triple $<h',r,t>$. Inspired by score function of TransE, we substitute the neighbor entities in $C_N(h)$ for the head $h'$ in triple to compute $a_i$ and then we define $a_i$
\begin{align} \label{a_i}
a_i = \|t_{n_i} - r_{n_i} + r - t\|
\end{align}

Where $t_{n_i}$ is the $ith$ neighbor entity in $C_N(h)$ and $r_{n_i}$ is the relation between $t_{n_i}$ and $h'$.

Then we use attention model $\alpha_i$ to represent how $h'$ selectively focuses on $C_N(h)$, If the value of $\alpha_i$ is higher, the corresponding neighbor entity in $C_N(h)$ is more important for $h'$.
\begin{equation}\label{alpha_i}
\alpha_i = \frac {\exp(-a_i)} {\sum_{j} \exp(-a_j)}
\end{equation}

In Eq.~\eqref{g_N}, given a neighbor context $C_N(h)$, the embedding vector of each neighbor has different weights by the attention mechanism in Eq.~\eqref{alpha_i}. Finally, based on the attention results we obtain the correlation between an arbitrary entity $h'$ and neighbor context $C_N(h)$:
\begin{align} \label{g_N}
g_N(h¡¯, C_N(h)) = -\sum_{i} \alpha_{i} \|\mathbf{h'} + \mathbf{r_{n_i}} - \mathbf{t_{n_i}}\|
\end{align}

The second part $P(r,t|C(h,r,t), h; \Theta)$ in Eq.~\eqref{decomposition} is the conditional probability that $t$ is the tail entity and $r$ is the relation given the head entity $h$, triple context and all the embeddings. In this part we introduce path context that means $t$ could be related to $h$ through a potential connective path in a knowledge graph. In the second part two kinds of relatedness should be considered that one is the relatedness between $h$ and $t$ in a a potential connective path $p_i$. And the other is the relatedness between $r$ and $p_i$.  Then We introduce path context among $h$ , $t$ and $r$ to measure the relatedness of them and approximate $P(r,t|C(h,r,t),h;\Theta)$ as $P(r,t|C_P(h,t),h;\Theta)$, where $C_P(h,t)$ is the \emph{path context} between $h$ and $t$. The approximated probability $P(r,t|C_P(h,t),$ $h;\Theta)$ is formalized as follows:

\begin{small}
\begin{equation}\label{P_t}
P(r,t|C_P(h, t), h;\Theta) \!=\! \frac {\exp(g_P(r,t, C_P(h, t)))} {\sum_{r' \in \mathcal{R}, t' \in \mathcal{E}} \exp(g_P(r', t', C_P(h, t)))}
\end{equation}
\end{small}

where $g_P(\cdot, \cdot)$ is a function of correlation among an arbitrary entity $t'$,an arbitrary relation $r'$ and path context of the specific entity pair $(h,t)$. Similar to the neighbor context, given a triple $<h,r,t>$ different pathes in a path context $C_P(h, t)$ have different power of influence the triple. For example when predicting the entity $Englis$, relations like $locate\_in\_Country$ will have less attentions, and the relation $Speak\_Language$  will have greater attention to represent $Engilsh$. In order to obtain $g_P(\cdot, \cdot)$, we firstly calculate the correlation $b_i$ between path $p_i$ in $C_P(h, t)$ and a tail entity:
\begin{equation}\label{f2}
b_i = \|\mathbf{h} + \mathbf{p_i} - \mathbf{t}\|
\end{equation}

Where $\bm{\mathrm{p_i}}$ composes all relations in $p_i$ into a single vector by summing over all their embeddings and this approach is also used in PTransE ~\cite{LinLLSRL15}. For example, for path $p_i=(r_{m_1}, \cdots, r_{m_i})$, the embedding of it is $\bm{\mathrm{p}}_i = \bm{\mathrm{r}}_{m_1} + \cdots + \bm{\mathrm{r}}_{m_i}$. Eq.~\eqref{f2} has a similar meaning with Eq.~\eqref{a_i}.

We then choose important pathes from $C_P(h, t)$ through the attention mechanisms. For each $p_i$, the weight $\beta_{i}$ is then defined as
\begin{align}\label{beta_i}
\beta_{i} = \frac {\exp(-b_i)} {\sum_j \exp(-b_j)}
\end{align}
If $p_i$ is more closer to $h$ and $t$, $\beta_{i}$ will indicate $p_i$ have greater attentions on translating from $h$ to $t$.

Finally given an  arbitrary entity $t'$ and an arbitrary relation $r'$, the correlation $g_P(\cdot, \cdot)$ between $t'$,$r'$ and $C_P(h, t)$ is:
\begin{equation}\label{gp}
g_P(r', t', C_P(h,t)) = -\sum_{i} \beta_i  (\|\mathbf{h} + \mathbf{p}_i - \mathbf{t'} \| + \|\mathbf{p}_i - \mathbf{r'}\|)
\end{equation}

In Eq.~\eqref{gp}, given a path context $C_P(h,t)$, the embedding vector of each path $p_i$ has different weight $\beta_i$. Eq.~\eqref{gp} has two parts, the first part is  $\|\mathbf{h} + \mathbf{p}_i - \mathbf{t'} \|$ that indicates the correlation between $t'$ and $p_i$. Similarly, the second part is $\|\mathbf{p}_i - \mathbf{r'}\|$ is the correlation between $r'$ and $p_i$. If $p_i$ is more related to $r'$ and $t'$, the probability of Eq.~\eqref{P_t} will be greater.

To utilize these two kinds of context, we combine them by jointly maximizing the probability in Eq.~\eqref{decomposition} of a triple which is exist in a knowledge graph. $P(h|C(h,r,t);\Theta)$ and $P(t,r|C(h,r,t),h;\Theta)$ can be approximated as $P(h|C_N(h);\Theta)$ and $P(r,t|C_P(h,t),h;\Theta)$, respectively. Thus Eq.~\eqref{decomposition} Thus, Eq.~\eqref{decomposition} can be approximated as:
\begin{equation}\label{decomposition_approx}
  f(h,r,t) \approx P(h|C_N(h);\Theta) \cdot P(r,t|C_P(h,t),h;\Theta)
\end{equation}
in which way the neighbor context and the path context of a triple are incorporated.
\subsection{Model Learning}
By feasible approximation, the score function is transformed to Eq.~\eqref{decomposition_approx}, each part is represented in softmax form as Eq.~\eqref{P_h} and Eq.~\eqref{P_t}. However, it is impractical to compute these softmax functions directly because of high computational overhead. Hence, we adopt negative sampling, which is proposed in \cite{DBLP:conf/nips/MikolovSCCD13} to approximate full softmax function efficiently, to approximate softmax functions in our model. For the whole knowledge graph $\mathcal{K}$, it is approximated via negative sampling as follows:

\begin{equation}
\label{t_score}
\begin{split}
P(\mathcal{K}|\Theta) &= \prod_{(h,r,t) \in \mathcal{K}} f(h,r,t) \\
&= \prod_{(h, r, t) \in \mathcal{K}} {P(h|C(h,r,t);\Theta) \cdot P(r,t|C(h,r,t), h; \Theta)} \\
&\approx \prod_{(h, r, t) \in \mathcal{K}} [\sigma(g_N(h, C_N(h))) \cdot \prod_{h'} \sigma(-g_N(h', C_N(h)))] \\
&\cdot [\sigma(g_P(r, t, C_P(h, t))) \cdot \prod_{r', t'} \sigma(- g_P(r', t', C_P(h, t)))]
\end{split}
\end{equation}

where $\{h', r, t\}$ is the corrupted triples by replacing the head entity with an arbitrary entity. $\{h, r', t'\}$  also is the corrupted triples by replacing the tail entity with an arbitrary entity and a relation $r$ with an arbitrary relation.  $\sigma(\cdot)$ is the logistic function. Simultaneity, for convenient application, we convert Eq.~\eqref{t_score} to the negative logarithm form which can be optimized by stochastic gradient descent (SGD). The objective function of a knowledge graph is formulated as:
\begin{equation}\label{approximation}
\begin{split}
-\log P(\mathcal{K} | \Theta)
& = -\sum_{(h,r,t) \in \mathcal{K}} [\log \sigma(g_N(h, C_N(h))) \\
&+ \sum_{h'} \log \sigma(-g_N(h', C_N(h))) \\
&+ \log \sigma(g_P(r, t, C_P(h, t))) \\
&+ \sum_{r', t'} \sigma(- g_P(r', t', C_P(h, t)))]
\end{split}
\end{equation}
In real data sets, the size of neighbor context and path context may be very large, which is computationally expensive for model learning. For this reason, we sample from neighbor context and path context to make triple context tractable. Specifically, we set a threshold $n_N$ for neighbor context and $n_P$ for path context; if the size of the original context exceeds the threshold, we sample a subset, size of which is the threshold, for model learning. Moreover, the length of relation path is constrained to 2 and 3 in our model.
\begin{comment}
the objective function is formulated as follows:
\begin{equation}\label{cost_function}
  \mathcal{L}(\mathcal{K}) = \sum_{(h,r,t)\in\mathcal{K}}
\end{equation}

For computational convenience, the original joint probability in Eq.~\eqref{joint_prob} is transformed into the negative logarithmic form, which can be optimized by stochastic gradient descent (SGD). The cost function of a triple $(h,r,t)$ is formulated as:
\begin{equation}\label{score_function}
*
\end{equation}
\end{comment}

